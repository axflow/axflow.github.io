---
title: Navigating the Shift
description: AI introduces a new way to build and iterate on products. In this non-deterministic world, engineering teams need new tools to systematically debug and improve their products.
authors:
  - nicholas
tags:
  - AI
  - LLM
  - Probabilistic Engineering
published: 2023-12-13T00:00:00.000Z
---
import { Image } from 'astro:assets';
import capabilities from '../../images/navigating-the-shift/capabilities.png'
import chevrolet from '../../images/navigating-the-shift/chevrolet.png'
import dataLoop from '../../images/navigating-the-shift/data-loop.png'
import fear from '../../images/navigating-the-shift/fear.png'
import graph from '../../images/navigating-the-shift/graph.png'
import studio from '../../images/navigating-the-shift/studio.webp'
import Link from '../../components/Link.astro';
export const components = {a: Link}


## From traditional software engineering to probabilistic AI engineering

Generative AI has taken the world by storm. ML has been around for decades, but recent breakthroughs in vision and text models have had an outsized impact on the landscape of tech.

Today, all it takes for an engineering team to introduce AI features into a product is a few API calls. The previous bar requiring  an in-house team of highly specialized ML engineers that trains models has been abstracted away. 

The barrier to entry is lower than ever, and so we find ourselves in the largest technology revolution that we‚Äôve seen in many years.

And so, just like software was [eating the world](https://a16z.com/why-software-is-eating-the-world/) in the 2010s, **AI is eating software** in the 2020s. Most companies have already introduced AI into their organization, and are aggressively exploring ways to leverage it to improve their products, reduce their costs, and make their workforce more productive.

This exciting change, like all change, comes with consequences. In particular, the role of software teams is undergoing a significant transformation. It‚Äôs changing both _how_ they write software and _what_ software they write. Most engineers at this point have experimented with the new APIs, for their employer or for themselves, yet they typically have stayed in the realm of prototyping. 

AI systems in production are a whole different game, they come with a unique set of challenges, and **require a shift both in mindset, and in tooling**.

<Image class="mx-auto rounded-sm" src={graph} alt="Graph of AI applications in production. Reliability over effort" width="600"/>

## Production AI is fear driven

Teams that are serious about shipping AI products go through two phases, in order:

1. The **capabilities phase**, driven by exciting possibilities of the positive cases
2. The **fear phase**, driven by the risks of the negative cases

These phases are separated by a very wide chasm (wider than nearly everyone thinks). Ever wondered why you see tons of _amazing_ demos all over, but very few robust applications that are actually shipped yet? Self driving is the perfect example: we had [self-driving cars prototypes](https://www.youtube.com/watch?v=ntIczNQKfjQ) driving around in 1986, but only in 2022 did Cruise get the first permit to let autonomous cars drive within a major US city (San Francisco). Cruise had to go from proving that the car can drive, to proving that it won‚Äôt do anything wrong, and the latter problem is significantly harder than the first. 

These phases aren't limited to safety critical environments. They affect all AI products: they are the result of confronting the probabilistic nature of AI with the infinite complexity of the real world.

### Capabilities phase

<Image class="float-right pt-4 pl-4 rounded-sm" src={capabilities} alt="The capabilities phase. Notice that the team will soon discover some troubling things about their spelling..." width="400"/>

Most companies are excitedly living their best lives in this phase right now when it comes to generative AI. Ever since GPT-3 came out, builders everywhere have come out with amazing demos and ideas of what these APIs can do. 

They can generate code! And amazing images! They can be specialized assistants like doctors or lawyers!

A lot of the existing tooling around AI, particualrly the tooling that we‚Äôve seen grow in popularity in the past year is optimized for this phase: frameworks, tools and APIs are all about what you can get the AI to _do_.

This capabilities phase, which can be assimilated to a research phase, is a critical part of the journey. It's fun and exciting, however beware of getting stuck here. It will not cut it if your goal is launching products in the real-world.


### The fear phase

<Image class="float-left pr-4 pt-4 rounded-sm" src={fear} alt="The fear phase" width="400"/>

**With production, comes fear.** When I worked at Cruise, the most important metric for the company (by a lot) was Safety Critical Events, and we wanted that number to be _as low as possible_. Similarly, most people building customer support chatbots, nutrition assistants, code generators that get close to launching enter a similar mindset: "How can I prevent the bad things.". Many are postponing their launches once they discover the issues that come with non-determinism: hallucinations, unstable product experience, brand risk, etc‚Ä¶

Imagine having your customer support chatbot keep mentioning COVID 19, because of how relevant that was at training time? Or trying to ensure that people only ask relevant questions, and not how to solve Navier-Stokes fluid equations (yes this happened, check below üëá).

<Image class="mx-auto rounded-sm" src={chevrolet} alt="Chevrolet customer support solves navier stokes" width="300"/>

This mindset shift might be hard, but it's expected. Once product builders start thinking this way, they are ready to move to the next step: **adapt their workflows and tooling** to help them conquer the fear, using a data driven approach. An approach which systematically detects outlying behavior, measures quality, and empowers teams to make changes without causing regressions (this one is truly critical). You need to accept that there will be edge-cases when the real-world collides with your AI system, and build confidence that you can systematically and continuously improve and patch the system without regressions.

## The continuous data loop

This is where the concept of a **continuous data loop** comes into play.

Unlike traditional software debugging and productivity, where issues are identified and fixed in a linear fashion, AI troubleshooting requires a probabilistic, data driven approach. When building AI systems, your team needs a systematic workflow which **leverages production data to continuously learn, and incrementally improve the overall system**. 

<Image class="mx-auto rounded-sm" src={dataLoop} alt="The continuous data loop of productive AI teams" width="600"/>

Intuitively, this should make sense: the more self-driving cars drive around the streets of a city, the better drivers they should become; the more a customer support chatbot solves production customer issues, the better it should become. In practice, this intuition requires a lot of work (and tooling) to manifest. You need tools which connect together the entire AI stack, tools which are designed to help you navigate a probabilistic environment (not a deterministic one). They need to empower teams to troubleshoot, form hypotheses, perform experiments, track metrics, evaluate if the experiment was successful, and integrate back with production to redeploy the better AI system.

Once equipped with such a workflow, teams can now do the previously impossible: systematically improve the business performance of the AI system, in a data-driven way. The tooling should help you:

- **Monitor**: find outliers in the data that are worth investigating
- **Troubleshoot & drill-down**: session replay, and visualization tools to understand what exactly went wrong and form a hypothesis on how this could be fixed
- **Manage datasets**: Examples of edge cases that hit production, labeled golden sets, specialized example sets, etc‚Ä¶
- **Modify with confidence**: Ability to change configuration, prompts, code, models, or anything about the system, and then test it against a baseline (typically, the existing production system).

A fantastic proxy metric of a company‚Äôs ‚ÄúAI system productivity‚Äù is how quickly they can go through this loop. If it takes a team a month to go through such an iteration cycle, they stand no chance compared to a competitor who can do it in a week, or a day.


## An example

Let‚Äôs walk through an example of a team building a customer chatbot using an LLM, which has all of the right tooling:

An anomaly detection tool has notified the team that there are some examples of suspicious behavior that were detected, and should be looked into.

The team easily loads these up in a production replay tool, and inspect them manually at first. They realize that there is an attempt from users to jailbreak the AI customer support chatbot to retrieve customer data. The AI is thankfully not leaking data, but still attempting to help these hackers as best it can, which isn't optimal behavior.

The team decides to add a `hack` class to their initial intent routing prompt, which sits at the root of their application. They seed it with a few examples taken from the anomly detection tool and introduce a new logical branch which shuts down the conversation politely when this happens.

Before redeploying the system, they run their integrated evaluation suites to make sure that changing the routing prompt didn‚Äôt have some unintended side effects. It turns out that it does mis-classify some requests from their test set, so they modify the prompt a second time to prevent that from happening, with some counter examples.

They re-run the evaluations, and are satisfied that there are no significant regressions, and feel comfortable deploying the new `hack` classifier to production.

They contribute back to the evaluations with some labeled examples of hack attempts and expected AI behavior on those cases, to monitor future regressions around this behavior.

Finally, they redeploy the system, and have setup an alert that notifies their visibility tooling when the classifier triggers. Indeed it catches a few attempts in the next few weeks, and seems to be working well. 

They know it‚Äôs not yet perfect, but as more data comes in and the adversaries adapt their strategy, the team knows that they in turn will also be able to adapt quickly.

## Axflow's mission

At Axflow, we're building the tooling for this data loop. Ben and I spent many years at Cruise building them already, and learning lessons the hard way while we did. 

Our mission now is to bring that tooling to all these new product teams building AI applications, and dramatically increase the development velocity of production AI systems.

We‚Äôve already shipped an [open-source TypeScript framework](https://github.com/axflow/axflow), and are now working on portions of the offline tooling (production session replay, drill-down capabilities, and dataset management) through our new product: [Axflow Studio](https://studio.axflow.dev).
<Image class="mx-auto rounded-sm" src={studio} alt="A screenshot of Axflow studio" width="600"/>

If you‚Äôre interested in up-leveling your production AI workflow, [reach out](https://calendly.com/nichochar/15min), we love talking to builders and learn about your use cases. We want to help your team improve their iteration speed, and ship amazing AI experiences!
